{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNdCsZ0zOYs8iyxrRt2tC4B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/taylan-sen/CIS355_FALL05/blob/main/Algorithmic_Bias_Simulation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4S3gt9ZWGwf",
        "outputId": "395a756a-c6df-422d-ddb7-fda0560ea389"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì¶ Installing required libraries... (this takes 2-3 minutes)\n",
            "============================================================\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m37.3/37.3 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\n",
            "‚úÖ Setup complete! Let's explore algorithmic fairness.\n",
            "\n",
            "============================================================\n",
            "üé¨ THE SCENARIO\n",
            "============================================================\n",
            "\n",
            "You are a Data Scientist at a hiring company. Your job is to build an AI \n",
            "system that predicts whether job applicants will earn high income (>$50K/year).\n",
            "\n",
            "Your company will use this to screen applicants for high-paying positions.\n",
            "\n",
            "THE PROBLEM: The training data comes from 1994 Census records. Historical\n",
            "hiring practices may have been biased. Will your AI learn and perpetuate \n",
            "that bias?\n",
            "\n",
            "Let's find out...\n",
            "\n",
            "\n",
            "============================================================\n",
            "üìä LOADING THE UCI ADULT INCOME DATASET\n",
            "============================================================\n",
            "\n",
            "‚úì Loaded 30,162 records from 1994 U.S. Census data\n",
            "\n",
            "üìã First few rows:\n",
            "   age         workclass  fnlwgt  education  education-num  \\\n",
            "0   39         State-gov   77516  Bachelors             13   \n",
            "1   50  Self-emp-not-inc   83311  Bachelors             13   \n",
            "2   38           Private  215646    HS-grad              9   \n",
            "3   53           Private  234721       11th              7   \n",
            "4   28           Private  338409  Bachelors             13   \n",
            "\n",
            "       marital-status         occupation   relationship   race     sex  \\\n",
            "0       Never-married       Adm-clerical  Not-in-family  White    Male   \n",
            "1  Married-civ-spouse    Exec-managerial        Husband  White    Male   \n",
            "2            Divorced  Handlers-cleaners  Not-in-family  White    Male   \n",
            "3  Married-civ-spouse  Handlers-cleaners        Husband  Black    Male   \n",
            "4  Married-civ-spouse     Prof-specialty           Wife  Black  Female   \n",
            "\n",
            "   capital-gain  capital-loss  hours-per-week native-country  income  \n",
            "0          2174             0              40  United-States       0  \n",
            "1             0             0              13  United-States       0  \n",
            "2             0             0              40  United-States       0  \n",
            "3             0             0              40  United-States       0  \n",
            "4             0             0              40           Cuba       0  \n",
            "\n",
            "============================================================\n",
            "üîç KEY QUESTION: Is this historical data biased?\n",
            "============================================================\n",
            "\n",
            "1Ô∏è‚É£ Income distribution by SEX:\n",
            "income  high_income_rate\n",
            "sex                     \n",
            "Female              11.4\n",
            "Male                31.4\n",
            "\n",
            "üí≠ REFLECTION QUESTION:\n",
            "   In 1994, men had ~3x higher rate of earning >$50K than women.\n",
            "   Does this reflect:\n",
            "   (a) Actual qualifications/merit?\n",
            "   (b) Historical discrimination?\n",
            "   (c) Systemic barriers (education access, childcare, etc.)?\n",
            "   (d) All of the above?\n",
            "\n",
            "2Ô∏è‚É£ Income distribution by RACE:\n",
            "race\n",
            "Asian-Pac-Islander    27.7\n",
            "White                 26.4\n",
            "Black                 13.0\n",
            "Amer-Indian-Eskimo    11.9\n",
            "Other                  9.1\n",
            "Name: income, dtype: float64\n",
            "\n",
            "‚ö†Ô∏è  ETHICAL WARNING:\n",
            "   If we train AI on this data, it will learn these patterns.\n",
            "   Question: Is that fair? Is it legal? Should we do it anyway?\n",
            "\n",
            "üëâ Press ENTER to continue to model training...\n",
            "\n",
            "============================================================\n",
            "üîß PREPARING DATA FOR AI TRAINING\n",
            "============================================================\n",
            "\n",
            "‚úì Training set: 21,113 people\n",
            "‚úì Test set: 9,049 people\n",
            "\n",
            "‚úì Features being used:\n",
            "   ‚Ä¢ age\n",
            "   ‚Ä¢ education-num\n",
            "   ‚Ä¢ hours-per-week\n",
            "   ‚Ä¢ capital-gain\n",
            "   ‚Ä¢ sex\n",
            "   ‚Ä¢ race\n",
            "\n",
            "============================================================\n",
            "ü§ñ SCENARIO 1: STANDARD AI (Maximum Accuracy)\n",
            "============================================================\n",
            "\n",
            "You train an AI to maximize accuracy. No fairness constraints.\n",
            "This is what most companies do by default...\n",
            "\n",
            "‚úì Overall Accuracy: 81.2%\n",
            "\n",
            "üìä FAIRNESS ANALYSIS: STANDARD MODEL\n",
            "------------------------------------------------------------\n",
            "\n",
            "1Ô∏è‚É£ SELECTION RATE (% predicted as high-income):\n",
            "   Female: 3.3%\n",
            "   Male:   20.2%\n",
            "\n",
            "   üìè Ratio (Female/Male): 0.16\n",
            "      ‚Ü≥ 1.0 = perfect parity, <0.8 = potential discrimination\n",
            "\n",
            "2Ô∏è‚É£ TRUE POSITIVE RATE (% of qualified people accepted):\n",
            "   Female: 22.6%\n",
            "   Male:   45.4%\n",
            "\n",
            "   üìè Difference: 22.8%\n",
            "\n",
            "3Ô∏è‚É£ CONFUSION MATRICES:\n",
            "\n",
            "   üë© FEMALE GROUP:\n",
            "      True Negative:  2558  |  False Positive:    21\n",
            "      False Negative:  257  |  True Positive:     75\n",
            "\n",
            "   üë® MALE GROUP:\n",
            "      True Negative:  3833  |  False Positive:   355\n",
            "      False Negative: 1064  |  True Positive:    886\n",
            "\n",
            "============================================================\n",
            "‚öñÔ∏è  ETHICAL ASSESSMENT\n",
            "============================================================\n",
            "\n",
            "LEGAL STANDARD (U.S. EEOC \"80% Rule\"):\n",
            "If selection rate for protected group < 80% of other group = potential discrimination\n",
            "\n",
            "YOUR MODEL'S STATUS:\n",
            "   ‚ùå FAILS 80% rule (16.3% < 80%)\n",
            "   ‚ö†Ô∏è  Could face discrimination lawsuit\n",
            "\n",
            "üí≠ DISCUSSION QUESTIONS:\n",
            "   1. The AI is 'just learning from data' - who is responsible for bias?\n",
            "   2. Is high accuracy worth potential discrimination?\n",
            "   3. At what point does deploying this become gross negligence?\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "ETHICS OF COMPUTING: ALGORITHMIC FAIRNESS LAB\n",
        "==============================================\n",
        "Using the UCI Adult Income Dataset\n",
        "\n",
        "LEARNING OBJECTIVES:\n",
        "- Understand how AI can perpetuate historical bias\n",
        "- Explore different definitions of \"fairness\"\n",
        "- Experience trade-offs between accuracy and fairness\n",
        "- Connect to real-world ethical dilemmas\n",
        "\n",
        "SETUP INSTRUCTIONS FOR GOOGLE COLAB:\n",
        "1. Go to colab.research.google.com\n",
        "2. File -> New Notebook\n",
        "3. Copy and paste this entire code\n",
        "4. Run each cell in order (Shift+Enter)\n",
        "\n",
        "Estimated Time: 60-90 minutes\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================================\n",
        "# PART 0: INSTALL AND IMPORT LIBRARIES\n",
        "# ============================================================================\n",
        "print(\"üì¶ Installing required libraries... (this takes 2-3 minutes)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "!pip install fairlearn scikit-learn pandas numpy matplotlib seaborn -q\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from fairlearn.metrics import MetricFrame, demographic_parity_ratio, equalized_odds_ratio\n",
        "from fairlearn.reductions import ExponentiatedGradient, DemographicParity, EqualizedOdds\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"\\n‚úÖ Setup complete! Let's explore algorithmic fairness.\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# PART 1: THE SCENARIO\n",
        "# ============================================================================\n",
        "print(\"=\" * 60)\n",
        "print(\"üé¨ THE SCENARIO\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\"\"\n",
        "You are a Data Scientist at a hiring company. Your job is to build an AI\n",
        "system that predicts whether job applicants will earn high income (>$50K/year).\n",
        "\n",
        "Your company will use this to screen applicants for high-paying positions.\n",
        "\n",
        "THE PROBLEM: The training data comes from 1994 Census records. Historical\n",
        "hiring practices may have been biased. Will your AI learn and perpetuate\n",
        "that bias?\n",
        "\n",
        "Let's find out...\n",
        "\"\"\")\n",
        "\n",
        "# ============================================================================\n",
        "# PART 2: LOAD AND EXPLORE THE DATA\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üìä LOADING THE UCI ADULT INCOME DATASET\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Load dataset\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
        "column_names = ['age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
        "                'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
        "                'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income']\n",
        "\n",
        "df = pd.read_csv(url, names=column_names, skipinitialspace=True, na_values='?')\n",
        "\n",
        "# Clean data\n",
        "df = df.dropna()\n",
        "df['income'] = df['income'].map({'<=50K': 0, '>50K': 1})\n",
        "\n",
        "print(f\"\\n‚úì Loaded {len(df):,} records from 1994 U.S. Census data\")\n",
        "print(\"\\nüìã First few rows:\")\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üîç KEY QUESTION: Is this historical data biased?\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Analyze by sex\n",
        "print(\"\\n1Ô∏è‚É£ Income distribution by SEX:\")\n",
        "sex_income = df.groupby(['sex', 'income']).size().unstack(fill_value=0)\n",
        "sex_income['high_income_rate'] = (sex_income[1] / (sex_income[0] + sex_income[1]) * 100).round(1)\n",
        "print(sex_income[['high_income_rate']])\n",
        "\n",
        "print(\"\\nüí≠ REFLECTION QUESTION:\")\n",
        "print(\"   In 1994, men had ~3x higher rate of earning >$50K than women.\")\n",
        "print(\"   Does this reflect:\")\n",
        "print(\"   (a) Actual qualifications/merit?\")\n",
        "print(\"   (b) Historical discrimination?\")\n",
        "print(\"   (c) Systemic barriers (education access, childcare, etc.)?\")\n",
        "print(\"   (d) All of the above?\")\n",
        "\n",
        "# Analyze by race\n",
        "print(\"\\n2Ô∏è‚É£ Income distribution by RACE:\")\n",
        "race_income = df.groupby('race')['income'].apply(lambda x: (x == 1).mean() * 100).round(1).sort_values(ascending=False)\n",
        "print(race_income)\n",
        "\n",
        "print(\"\\n‚ö†Ô∏è  ETHICAL WARNING:\")\n",
        "print(\"   If we train AI on this data, it will learn these patterns.\")\n",
        "print(\"   Question: Is that fair? Is it legal? Should we do it anyway?\")\n",
        "\n",
        "input(\"\\nüëâ Press ENTER to continue to model training...\")\n",
        "\n",
        "# ============================================================================\n",
        "# PART 3: PREPARE DATA FOR MACHINE LEARNING\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üîß PREPARING DATA FOR AI TRAINING\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Select features\n",
        "features_to_use = ['age', 'education-num', 'hours-per-week', 'capital-gain', 'sex', 'race']\n",
        "X = df[features_to_use].copy()\n",
        "y = df['income']\n",
        "\n",
        "# Convert categorical to numbers\n",
        "X['sex'] = (X['sex'] == 'Male').astype(int)\n",
        "X['race'] = (X['race'] == 'White').astype(int)  # Binary for simplicity\n",
        "\n",
        "# Store sensitive attribute for fairness analysis\n",
        "sensitive_features = X['sex'].copy()  # 1=Male, 0=Female\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test, sf_train, sf_test = train_test_split(\n",
        "    X, y, sensitive_features, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úì Training set: {len(X_train):,} people\")\n",
        "print(f\"‚úì Test set: {len(X_test):,} people\")\n",
        "print(\"\\n‚úì Features being used:\")\n",
        "for col in features_to_use:\n",
        "    print(f\"   ‚Ä¢ {col}\")\n",
        "\n",
        "# ============================================================================\n",
        "# PART 4: TRAIN THE \"STANDARD\" AI (No Fairness Constraints)\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ü§ñ SCENARIO 1: STANDARD AI (Maximum Accuracy)\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nYou train an AI to maximize accuracy. No fairness constraints.\")\n",
        "print(\"This is what most companies do by default...\\n\")\n",
        "\n",
        "# Train model\n",
        "model_standard = LogisticRegression(max_iter=1000, random_state=42)\n",
        "model_standard.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_standard = model_standard.predict(X_test)\n",
        "\n",
        "# Overall accuracy\n",
        "accuracy_standard = accuracy_score(y_test, y_pred_standard)\n",
        "print(f\"‚úì Overall Accuracy: {accuracy_standard:.1%}\")\n",
        "\n",
        "# Fairness metrics\n",
        "def analyze_fairness(y_true, y_pred, sensitive_feature, model_name):\n",
        "    print(f\"\\nüìä FAIRNESS ANALYSIS: {model_name}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    # Selection rates by group\n",
        "    mf = MetricFrame(\n",
        "        metrics=lambda y_t, y_p: y_p.mean(),\n",
        "        y_true=y_true,\n",
        "        y_pred=y_pred,\n",
        "        sensitive_features=sensitive_feature\n",
        "    )\n",
        "\n",
        "    print(\"\\n1Ô∏è‚É£ SELECTION RATE (% predicted as high-income):\")\n",
        "    print(f\"   Female: {mf.by_group[0]:.1%}\")\n",
        "    print(f\"   Male:   {mf.by_group[1]:.1%}\")\n",
        "\n",
        "    selection_ratio = mf.by_group[0] / mf.by_group[1]\n",
        "    print(f\"\\n   üìè Ratio (Female/Male): {selection_ratio:.2f}\")\n",
        "    print(f\"      ‚Ü≥ 1.0 = perfect parity, <0.8 = potential discrimination\")\n",
        "\n",
        "    # True positive rates (qualified people who get hired)\n",
        "    female_mask = sensitive_feature == 0\n",
        "    male_mask = sensitive_feature == 1\n",
        "\n",
        "    female_tpr = ((y_pred[female_mask] == 1) & (y_true[female_mask] == 1)).sum() / (y_true[female_mask] == 1).sum()\n",
        "    male_tpr = ((y_pred[male_mask] == 1) & (y_true[male_mask] == 1)).sum() / (y_true[male_mask] == 1).sum()\n",
        "\n",
        "    print(\"\\n2Ô∏è‚É£ TRUE POSITIVE RATE (% of qualified people accepted):\")\n",
        "    print(f\"   Female: {female_tpr:.1%}\")\n",
        "    print(f\"   Male:   {male_tpr:.1%}\")\n",
        "    print(f\"\\n   üìè Difference: {abs(female_tpr - male_tpr):.1%}\")\n",
        "\n",
        "    # Confusion matrix by group\n",
        "    print(\"\\n3Ô∏è‚É£ CONFUSION MATRICES:\")\n",
        "    print(\"\\n   üë© FEMALE GROUP:\")\n",
        "    cm_female = confusion_matrix(y_true[female_mask], y_pred[female_mask])\n",
        "    print(f\"      True Negative: {cm_female[0,0]:>5}  |  False Positive: {cm_female[0,1]:>5}\")\n",
        "    print(f\"      False Negative: {cm_female[1,0]:>4}  |  True Positive: {cm_female[1,1]:>6}\")\n",
        "\n",
        "    print(\"\\n   üë® MALE GROUP:\")\n",
        "    cm_male = confusion_matrix(y_true[male_mask], y_pred[male_mask])\n",
        "    print(f\"      True Negative: {cm_male[0,0]:>5}  |  False Positive: {cm_male[0,1]:>5}\")\n",
        "    print(f\"      False Negative: {cm_male[1,0]:>4}  |  True Positive: {cm_male[1,1]:>6}\")\n",
        "\n",
        "    return selection_ratio, female_tpr, male_tpr\n",
        "\n",
        "ratio1, f_tpr1, m_tpr1 = analyze_fairness(y_test.values, y_pred_standard, sf_test.values, \"STANDARD MODEL\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"‚öñÔ∏è  ETHICAL ASSESSMENT\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\"\"\n",
        "LEGAL STANDARD (U.S. EEOC \"80% Rule\"):\n",
        "If selection rate for protected group < 80% of other group = potential discrimination\n",
        "\n",
        "YOUR MODEL'S STATUS:\"\"\")\n",
        "if ratio1 < 0.8:\n",
        "    print(f\"   ‚ùå FAILS 80% rule ({ratio1:.1%} < 80%)\")\n",
        "    print(\"   ‚ö†Ô∏è  Could face discrimination lawsuit\")\n",
        "else:\n",
        "    print(f\"   ‚úÖ Passes 80% rule ({ratio1:.1%} ‚â• 80%)\")\n",
        "\n",
        "print(\"\\nüí≠ DISCUSSION QUESTIONS:\")\n",
        "print(\"   1. The AI is 'just learning from data' - who is responsible for bias?\")\n",
        "print(\"   2. Is high accuracy worth potential discrimination?\")\n",
        "print(\"   3. At what point does deploying this become gross negligence?\")\n",
        "\n",
        "input(\"\\nüëâ Press ENTER to see fairness interventions...\")\n",
        "\n",
        "# ============================================================================\n",
        "# PART 5: TRAIN WITH DEMOGRAPHIC PARITY CONSTRAINT\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ü§ñ SCENARIO 2: DEMOGRAPHIC PARITY (Equal Selection Rates)\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nYou add a fairness constraint: EQUAL selection rates for both groups.\")\n",
        "print(\"This means the % of males and females predicted as high-income must be equal.\\n\")\n",
        "\n",
        "# Train fair model\n",
        "constraint_dp = DemographicParity()\n",
        "mitigator_dp = ExponentiatedGradient(model_standard, constraint_dp)\n",
        "mitigator_dp.fit(X_train, y_train, sensitive_features=sf_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_dp = mitigator_dp.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "accuracy_dp = accuracy_score(y_test, y_pred_dp)\n",
        "print(f\"‚úì Overall Accuracy: {accuracy_dp:.1%}\")\n",
        "print(f\"   ‚Ü≥ Change from standard: {accuracy_dp - accuracy_standard:+.1%}\")\n",
        "\n",
        "ratio2, f_tpr2, m_tpr2 = analyze_fairness(y_test.values, y_pred_dp, sf_test.values, \"DEMOGRAPHIC PARITY MODEL\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"‚öñÔ∏è  TRADE-OFF ANALYSIS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\n‚úÖ GAINED: Equal selection rates (ratio now ‚âà {ratio2:.2f})\")\n",
        "print(f\"‚ùå LOST: Accuracy decreased by {(accuracy_standard - accuracy_dp):.1%}\")\n",
        "print(\"\\nüí≠ DISCUSSION QUESTIONS:\")\n",
        "print(\"   1. Is this 'reverse discrimination' against qualified males?\")\n",
        "print(\"   2. Who bears the cost of this fairness intervention?\")\n",
        "print(\"   3. Should we sacrifice accuracy for fairness? How much?\")\n",
        "\n",
        "input(\"\\nüëâ Press ENTER to see the third approach...\")\n",
        "\n",
        "# ============================================================================\n",
        "# PART 6: TRAIN WITH EQUALIZED ODDS CONSTRAINT\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ü§ñ SCENARIO 3: EQUALIZED ODDS (Equal Treatment of Qualified)\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nYou try a different fairness constraint: EQUAL OPPORTUNITY.\")\n",
        "print(\"Qualified candidates should be accepted at equal rates, regardless of group.\\n\")\n",
        "\n",
        "# Train fair model\n",
        "constraint_eo = EqualizedOdds()\n",
        "mitigator_eo = ExponentiatedGradient(model_standard, constraint_eo)\n",
        "mitigator_eo.fit(X_train, y_train, sensitive_features=sf_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_eo = mitigator_eo.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "accuracy_eo = accuracy_score(y_test, y_pred_eo)\n",
        "print(f\"‚úì Overall Accuracy: {accuracy_eo:.1%}\")\n",
        "print(f\"   ‚Ü≥ Change from standard: {accuracy_eo - accuracy_standard:+.1%}\")\n",
        "\n",
        "ratio3, f_tpr3, m_tpr3 = analyze_fairness(y_test.values, y_pred_eo, sf_test.values, \"EQUALIZED ODDS MODEL\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"‚öñÔ∏è  COMPARISON OF ALL THREE APPROACHES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Approach': ['Standard (Max Accuracy)', 'Demographic Parity', 'Equalized Odds'],\n",
        "    'Accuracy': [f\"{accuracy_standard:.1%}\", f\"{accuracy_dp:.1%}\", f\"{accuracy_eo:.1%}\"],\n",
        "    'Selection Ratio (F/M)': [f\"{ratio1:.2f}\", f\"{ratio2:.2f}\", f\"{ratio3:.2f}\"],\n",
        "    'Female TPR': [f\"{f_tpr1:.1%}\", f\"{f_tpr2:.1%}\", f\"{f_tpr3:.1%}\"],\n",
        "    'Male TPR': [f\"{m_tpr1:.1%}\", f\"{m_tpr2:.1%}\", f\"{m_tpr3:.1%}\"],\n",
        "})\n",
        "\n",
        "print(\"\\n\")\n",
        "print(comparison_df.to_string(index=False))\n",
        "\n",
        "print(\"\\nüìä Visualizing the trade-offs...\")\n",
        "\n",
        "# ============================================================================\n",
        "# PART 7: VISUALIZE THE FAIRNESS-ACCURACY TRADE-OFF\n",
        "# ============================================================================\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# Plot 1: Accuracy comparison\n",
        "approaches = ['Standard', 'Demographic\\nParity', 'Equalized\\nOdds']\n",
        "accuracies = [accuracy_standard, accuracy_dp, accuracy_eo]\n",
        "colors = ['red', 'orange', 'green']\n",
        "\n",
        "axes[0].bar(approaches, accuracies, color=colors, alpha=0.7)\n",
        "axes[0].set_ylabel('Accuracy', fontsize=12)\n",
        "axes[0].set_title('Accuracy Comparison', fontsize=14, fontweight='bold')\n",
        "axes[0].set_ylim([0.7, 0.9])\n",
        "for i, v in enumerate(accuracies):\n",
        "    axes[0].text(i, v + 0.01, f'{v:.1%}', ha='center', fontweight='bold')\n",
        "axes[0].axhline(y=0.8, color='gray', linestyle='--', alpha=0.5)\n",
        "\n",
        "# Plot 2: Selection rate ratio\n",
        "ratios = [ratio1, ratio2, ratio3]\n",
        "axes[1].bar(approaches, ratios, color=colors, alpha=0.7)\n",
        "axes[1].set_ylabel('Selection Ratio (Female/Male)', fontsize=12)\n",
        "axes[1].set_title('Fairness: Selection Parity', fontsize=14, fontweight='bold')\n",
        "axes[1].axhline(y=1.0, color='green', linestyle='--', linewidth=2, label='Perfect Parity')\n",
        "axes[1].axhline(y=0.8, color='red', linestyle='--', linewidth=2, label='Legal Threshold')\n",
        "axes[1].legend()\n",
        "axes[1].set_ylim([0, 1.2])\n",
        "for i, v in enumerate(ratios):\n",
        "    axes[1].text(i, v + 0.05, f'{v:.2f}', ha='center', fontweight='bold')\n",
        "\n",
        "# Plot 3: True positive rate gap\n",
        "tpr_gaps = [abs(f_tpr1 - m_tpr1), abs(f_tpr2 - m_tpr2), abs(f_tpr3 - m_tpr3)]\n",
        "axes[2].bar(approaches, tpr_gaps, color=colors, alpha=0.7)\n",
        "axes[2].set_ylabel('TPR Gap (|Female - Male|)', fontsize=12)\n",
        "axes[2].set_title('Fairness: Equal Opportunity', fontsize=14, fontweight='bold')\n",
        "axes[2].set_ylim([0, 0.15])\n",
        "for i, v in enumerate(tpr_gaps):\n",
        "    axes[2].text(i, v + 0.005, f'{v:.1%}', ha='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('fairness_tradeoffs.png', dpi=150, bbox_inches='tight')\n",
        "print(\"‚úì Saved visualization as 'fairness_tradeoffs.png'\")\n",
        "plt.show()\n",
        "\n",
        "# ============================================================================\n",
        "# PART 8: THE IMPOSSIBLE THEOREM\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üö´ THE IMPOSSIBILITY THEOREM\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\"\"\n",
        "MATHEMATICAL FACT: You cannot satisfy all fairness criteria simultaneously.\n",
        "\n",
        "For example, you CANNOT have both:\n",
        "  1. Demographic Parity (equal selection rates)\n",
        "  AND\n",
        "  2. Equal Opportunity (equal true positive rates)\n",
        "\n",
        "...unless the base rates are identical between groups (which they never are).\n",
        "\n",
        "This means: SOMEONE ALWAYS GETS DISADVANTAGED by your choice of fairness metric.\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\nüí≠ ETHICAL QUESTION:\")\n",
        "print(\"   If perfect fairness is mathematically impossible,\")\n",
        "print(\"   who should decide which definition of 'fair' we use?\")\n",
        "print(\"\\n   Options:\")\n",
        "print(\"   (a) The company? (conflict of interest)\")\n",
        "print(\"   (b) The government? (regulation)\")\n",
        "print(\"   (c) The affected communities? (democratic input)\")\n",
        "print(\"   (d) AI ethicists? (expert judgment)\")\n",
        "\n",
        "# ============================================================================\n",
        "# PART 9: REAL-WORLD CONNECTION\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üåç REAL-WORLD CASE STUDIES\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\"\"\n",
        "1. AMAZON'S RESUME SCREENER (2018)\n",
        "   ‚Ä¢ Trained on 10 years of hiring data (mostly male engineers)\n",
        "   ‚Ä¢ AI learned to penalize resumes with word \"women's\" (e.g., \"women's chess club\")\n",
        "   ‚Ä¢ Downranked graduates of all-women's colleges\n",
        "   ‚Ä¢ Amazon scrapped the project\n",
        "   ‚Ä¢ Question: Was this gross negligence?\n",
        "\n",
        "2. COMPAS RECIDIVISM ALGORITHM (Criminal Justice)\n",
        "   ‚Ä¢ Used to predict re-offending risk\n",
        "   ‚Ä¢ ProPublica investigation: Black defendants falsely labeled high-risk 2x more\n",
        "   ‚Ä¢ Company claimed algorithm was \"race-blind\"\n",
        "   ‚Ä¢ Still widely used despite controversy\n",
        "   ‚Ä¢ Question: Is it worse to have biased AI or biased humans?\n",
        "\n",
        "3. HIREVUE VIDEO INTERVIEW AI (2019-2021)\n",
        "   ‚Ä¢ Analyzed facial expressions and speech patterns\n",
        "   ‚Ä¢ Criticized for disability discrimination and lack of transparency\n",
        "   ‚Ä¢ Illinois banned it in 2020\n",
        "   ‚Ä¢ Company pivoted away from facial analysis\n",
        "   ‚Ä¢ Question: Should AI be allowed to judge humans on things we don't understand?\n",
        "\n",
        "4. APPLE CARD CREDIT LIMITS (2019)\n",
        "   ‚Ä¢ Viral tweet: Man offered 20x higher limit than his wife (same income/assets)\n",
        "   ‚Ä¢ Apple claimed algorithm was \"fair\" (no explicit gender variable)\n",
        "   ‚Ä¢ Investigation revealed indirect discrimination through other features\n",
        "   ‚Ä¢ Question: Is it discrimination if the AI didn't explicitly use protected attributes?\n",
        "\"\"\")\n",
        "\n",
        "# ============================================================================\n",
        "# PART 10: YOUR DECISION\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üéØ YOUR FINAL DECISION\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\"\"\n",
        "You are the CTO. Your board is demanding you deploy the hiring AI next month.\n",
        "\n",
        "You have three options:\n",
        "  [1] Standard Model (85% accurate, fails legal fairness test)\n",
        "  [2] Demographic Parity (78% accurate, equal selection rates)\n",
        "  [3] Equalized Odds (82% accurate, equal treatment of qualified)\n",
        "\n",
        "Each has legal, ethical, and business implications.\n",
        "\"\"\")\n",
        "\n",
        "choice = input(\"\\nWhich model do you deploy? [1/2/3]: \").strip()\n",
        "\n",
        "consequences = {\n",
        "    '1': \"\"\"\n",
        "    ‚öñÔ∏è  SIX MONTHS LATER:\n",
        "\n",
        "    Your company is sued for discrimination. The EEOC investigation reveals:\n",
        "    ‚Ä¢ Female applicants were rejected at 2.5x the rate of equally qualified males\n",
        "    ‚Ä¢ Your data science team had flagged this risk in internal memos\n",
        "    ‚Ä¢ You deployed anyway to maximize accuracy\n",
        "\n",
        "    Legal assessment: GROSS NEGLIGENCE\n",
        "    ‚Ä¢ You had actual knowledge of the bias\n",
        "    ‚Ä¢ You proceeded with reckless disregard for harm\n",
        "    ‚Ä¢ Settlement: $47 million + mandatory audit for 5 years\n",
        "\n",
        "    Personal consequences: You are fired. Your professional reputation is destroyed.\n",
        "    The case becomes a Harvard Business School case study on AI ethics failures.\n",
        "    \"\"\",\n",
        "    '2': \"\"\"\n",
        "    ‚öñÔ∏è  SIX MONTHS LATER:\n",
        "\n",
        "    Mixed results:\n",
        "    ‚úÖ No discrimination lawsuits\n",
        "    ‚úÖ Diversity metrics improve by 40%\n",
        "    ‚úÖ Company praised by advocacy groups\n",
        "\n",
        "    ‚ùå Several highly qualified candidates (both male and female) were rejected\n",
        "       to maintain equal rates\n",
        "    ‚ùå Accuracy drop led to 12% more poor hires\n",
        "    ‚ùå CFO questions if the cost was worth it\n",
        "    ‚ùå Some employees claim \"reverse discrimination\"\n",
        "\n",
        "    You keep your job, but face criticism from multiple sides. The fairness vs.\n",
        "    accuracy debate continues. At least you can sleep at night knowing you tried.\n",
        "    \"\"\",\n",
        "    '3': \"\"\"\n",
        "    ‚öñÔ∏è  SIX MONTHS LATER:\n",
        "\n",
        "    Reasonable compromise:\n",
        "    ‚úÖ Qualified candidates from both groups accepted at equal rates\n",
        "    ‚úÖ Lawsuit risk is manageable\n",
        "    ‚úÖ Only modest accuracy loss\n",
        "    ‚úÖ Board accepts this as the \"least bad\" option\n",
        "\n",
        "    ‚ö†Ô∏è  Selection rates still differ between groups (which draws some criticism)\n",
        "    ‚ö†Ô∏è  Some argue this doesn't address systemic barriers\n",
        "\n",
        "    You keep your job. The AI system becomes an industry benchmark for\n",
        "    \"responsible AI deployment.\" You're invited to speak at conferences\n",
        "    about navigating fairness trade-offs.\n",
        "    \"\"\"\n",
        "}\n",
        "\n",
        "print(consequences.get(choice, \"\\n‚ùå Invalid choice. But in the real world, doing nothing is also a choice...\"))\n",
        "\n",
        "# ============================================================================\n",
        "# PART 11: DISCUSSION QUESTIONS FOR CLASS\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üí≠ DISCUSSION QUESTIONS FOR YOUR CLASS\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\"\"\n",
        "1. RESPONSIBILITY & NEGLIGENCE\n",
        "   ‚Ä¢ At what point does deploying biased AI become gross negligence?\n",
        "   ‚Ä¢ Who is liable: the CEO? CTO? Data scientist? The company?\n",
        "   ‚Ä¢ Should individuals face criminal charges for AI harms?\n",
        "\n",
        "2. FAIRNESS DEFINITIONS\n",
        "   ‚Ä¢ Which fairness metric matters most? Why?\n",
        "   ‚Ä¢ Who should decide: engineers, lawyers, ethicists, affected communities?\n",
        "   ‚Ä¢ Can a \"technical\" solution ever solve a social problem?\n",
        "\n",
        "3. TRADE-OFFS\n",
        "   ‚Ä¢ How much accuracy should we sacrifice for fairness?\n",
        "   ‚Ä¢ Who bears the cost of fairness interventions?\n",
        "   ‚Ä¢ Is it ethical to reduce overall performance to help one group?\n",
        "\n",
        "4. SYSTEMIC ISSUES\n",
        "   ‚Ä¢ If all historical data is biased, can we ever train fair AI?\n",
        "   ‚Ä¢ Should we use AI for decisions we don't fully understand?\n",
        "   ‚Ä¢ Does AI make discrimination worse, or just make it visible?\n",
        "\n",
        "5. POLICY & REGULATION\n",
        "   ‚Ä¢ Should hiring algorithms be regulated? How?\n",
        "   ‚Ä¢ Should companies be required to audit AI for bias?\n",
        "   ‚Ä¢ What about AI used in criminal justice, lending, healthcare?\n",
        "\n",
        "6. CONNECTION TO OTHER TOPICS\n",
        "   ‚Ä¢ How does this relate to the \"racist sinks\" example?\n",
        "   ‚Ä¢ What about the dermatology AI that looked for rulers?\n",
        "   ‚Ä¢ How does this connect to the AI 2027 scenario and alignment?\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"‚úÖ LAB COMPLETE!\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\"\"\n",
        "You've experienced firsthand the ethical challenges of AI fairness.\n",
        "\n",
        "KEY TAKEAWAYS:\n",
        "‚Ä¢ AI learns and amplifies bias from historical data\n",
        "‚Ä¢ Perfect fairness is mathematically impossible\n",
        "‚Ä¢ Every choice disadvantages someone\n",
        "‚Ä¢ Technical solutions alone can't solve social problems\n",
        "‚Ä¢ Transparency and accountability matter\n",
        "\n",
        "NEXT STEPS:\n",
        "‚Ä¢ Discuss your results with classmates\n",
        "‚Ä¢ Research real-world cases (Amazon, COMPAS, etc.)\n",
        "‚Ä¢ Think about: what regulations should exist?\n",
        "‚Ä¢ Consider: would YOU want to work on AI ethics?\n",
        "\n",
        "Thank you for engaging seriously with these difficult questions. üôè\n",
        "\"\"\")\n",
        "\n",
        "# Optional: Save results for later analysis\n",
        "results_summary = {\n",
        "    'standard_accuracy': accuracy_standard,\n",
        "    'dp_accuracy': accuracy_dp,\n",
        "    'eo_accuracy': accuracy_eo,\n",
        "    'standard_ratio': ratio1,\n",
        "    'dp_ratio': ratio2,\n",
        "    'eo_ratio': ratio3,\n",
        "    'choice': choice\n",
        "}\n",
        "\n",
        "print(\"\\nüìÅ Results saved to variable 'results_summary'\")\n",
        "print(\"   You can access individual values like: results_summary['standard_accuracy']\")"
      ]
    }
  ]
}